
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Classification with XGBoost &#8212; Machine Learning Scientist with Python</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Regression with XGBoost" href="Extreme-Gradient-Boosting-with-XGBoost-2.html" />
    <link rel="prev" title="Extreme Gradient Boosting with XGBoost" href="Extreme-Gradient-Boosting-with-XGBoost-0.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Scientist with Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Supervised-Learning-with-scikit-learn-0.html">
   Supervised Learning with scikit-learn
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Supervised-Learning-with-scikit-learn-1.html">
     Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Supervised-Learning-with-scikit-learn-2.html">
     Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Supervised-Learning-with-scikit-learn-3.html">
     Fine-Tuning Your Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Supervised-Learning-with-scikit-learn-4.html">
     Preprocessing and Pipelines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Unsupervised-Learning-in-Python-0.html">
   Unsupervised Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Unsupervised-Learning-in-Python-1.html">
     Clustering for dataset exploration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Unsupervised-Learning-in-Python-2.html">
     Visualization with hierarchical clustering and t-SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Unsupervised-Learning-in-Python-3.html">
     Decorrelating your data and dimension reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Unsupervised-Learning-in-Python-4.html">
     Discovering interpretable features
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Linear-Classifiers-in-Python-0.html">
   Linear Classifiers in Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear-Classifiers-in-Python-1.html">
     Applying logistic regression and SVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear-Classifiers-in-Python-2.html">
     Loss functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear-Classifiers-in-Python-3.html">
     Logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear-Classifiers-in-Python-4.html">
     Support Vector Machines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-0.html">
   Machine Learning with Tree-Based Models in Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-1.html">
     Classification and Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-2.html">
     The Bias-Variance Tradeoff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-3.html">
     Bagging and Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-4.html">
     Boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-5.html">
     Model Tuning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="Extreme-Gradient-Boosting-with-XGBoost-0.html">
   Extreme Gradient Boosting with XGBoost
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Classification with XGBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Extreme-Gradient-Boosting-with-XGBoost-2.html">
     Regression with XGBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Extreme-Gradient-Boosting-with-XGBoost-3.html">
     Fine-tuning your XGBoost model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Extreme-Gradient-Boosting-with-XGBoost-4.html">
     Using XGBoost in pipelines
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/iuvnumath/datacamp-python-ml"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/iuvnumath/datacamp-python-ml/issues/new?title=Issue%20on%20page%20%2FExtreme-Gradient-Boosting-with-XGBoost-1.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/Extreme-Gradient-Boosting-with-XGBoost-1.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#welcome-to-the-course">
   Welcome to the course!
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#which-of-these-is-a-classification-problem">
     Which of these is a classification problem?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#which-of-these-is-a-binary-classification-problem">
     Which of these is a binary classification problem?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introducing-xgboost">
   Introducing XGBoost
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#xgboost-fit-predict">
     XGBoost: Fit/Predict
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-a-decision-tree">
   What is a decision tree?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-trees">
     Decision trees
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-boosting">
   What is Boosting?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measuring-accuracy">
     Measuring accuracy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measuring-auc">
     Measuring AUC
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#when-should-i-use-xgboost">
   When should I use XGBoost?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-xgboost">
     Using XGBoost
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Classification with XGBoost</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#welcome-to-the-course">
   Welcome to the course!
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#which-of-these-is-a-classification-problem">
     Which of these is a classification problem?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#which-of-these-is-a-binary-classification-problem">
     Which of these is a binary classification problem?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introducing-xgboost">
   Introducing XGBoost
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#xgboost-fit-predict">
     XGBoost: Fit/Predict
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-a-decision-tree">
   What is a decision tree?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-trees">
     Decision trees
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-boosting">
   What is Boosting?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measuring-accuracy">
     Measuring accuracy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measuring-auc">
     Measuring AUC
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#when-should-i-use-xgboost">
   When should I use XGBoost?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-xgboost">
     Using XGBoost
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="classification-with-xgboost">
<h1>Classification with XGBoost<a class="headerlink" href="#classification-with-xgboost" title="Permalink to this headline">#</a></h1>
<p class="chapter__description">
This chapter will introduce you to the fundamental idea behind
XGBoost—boosted learners. Once you understand how XGBoost works, you’ll
apply it to solve a common classification problem found in industry:
predicting whether a customer will stop being a customer at some point
in the future.
</p>
<section id="welcome-to-the-course">
<h2>Welcome to the course!<a class="headerlink" href="#welcome-to-the-course" title="Permalink to this headline">#</a></h2>
<section id="which-of-these-is-a-classification-problem">
<h3>Which of these is a classification problem?<a class="headerlink" href="#which-of-these-is-a-classification-problem" title="Permalink to this headline">#</a></h3>
<p>
Given below are 4 potential machine learning problems you might
encounter in the wild. Pick the one that is a classification problem.
</p>
<li>
Given past performance of stocks and various other financial data,
predicting the exact price of a given stock (Google) tomorrow.
</li>
<li>
Given a large dataset of user behaviors on a website, generating an
informative segmentation of the users based on their behaviors.
</li>
<strong>
<li>
Predicting whether a given user will click on an ad given the ad content
and metadata associated with the user.
</li>
</strong>
<li>
Given a user’s past behavior on a video platform, presenting him/her
with a series of recommended videos to watch next.
</li>
<p class="dc-completion-pane__message dc-u-maxw-100pc">
Well done! This is indeed a classification problem.
</p>
</section>
<section id="which-of-these-is-a-binary-classification-problem">
<h3>Which of these is a binary classification problem?<a class="headerlink" href="#which-of-these-is-a-binary-classification-problem" title="Permalink to this headline">#</a></h3>
<p>
Great! A classification problem involves predicting the category a given
data point belongs to out of a finite set of possible categories.
Depending on how many possible categories there are to predict, a
classification problem can be either binary or multi-class. Let’s do
another quick refresher here. Your job is to pick the
<strong>binary</strong> classification problem out of the following list
of supervised learning problems.
</p>
<strong>
<li>
Predicting whether a given image contains a cat.
</li>
</strong>
<li>
Predicting the emotional valence of a sentence (Valence can be positive,
negative, or neutral).
</li>
<li>
Recommending the most tax-efficient strategy for tax filing in an
automated accounting system.
</li>
<li>
Given a list of symptoms, generating a rank-ordered list of most likely
diseases.
</li>
<p class="dc-completion-pane__message dc-u-maxw-100pc">
Correct! A binary classification problem involves picking between 2
choices.
</p>
</section>
</section>
<section id="introducing-xgboost">
<h2>Introducing XGBoost<a class="headerlink" href="#introducing-xgboost" title="Permalink to this headline">#</a></h2>
<section id="xgboost-fit-predict">
<h3>XGBoost: Fit/Predict<a class="headerlink" href="#xgboost-fit-predict" title="Permalink to this headline">#</a></h3>
<p>
It’s time to create your first XGBoost model! As Sergey showed you in
the video, you can use the scikit-learn <code>.fit()</code> /
<code>.predict()</code> paradigm that you are already familiar to build
your XGBoost models, as the <code>xgboost</code> library has a
scikit-learn compatible API!
</p>
<p>
Here, you’ll be working with churn data. This dataset contains imaginary
data from a ride-sharing app with user behaviors over their first month
of app usage in a set of imaginary cities as well as whether they used
the service 5 months after sign-up. It has been pre-loaded for you into
a DataFrame called <code>churn_data</code> - explore it in the Shell!
</p>
<p>
Your goal is to use the first month’s worth of data to predict whether
the app’s users will remain users of the service at the 5 month mark.
This is a typical setup for a churn prediction problem. To do this,
you’ll split the data into training and test sets, fit a small
<code>xgboost</code> model on the training set, and evaluate its
performance on the test set by computing its accuracy.
</p>
<p>
<code>pandas</code> and <code>numpy</code> have been imported as
<code>pd</code> and <code>np</code>, and <code>train_test_split</code>
has been imported from <code>sklearn.model_selection</code>.
Additionally, the arrays for the features and the target have been
created as <code>X</code> and <code>y</code>.
</p>
<li>
Import <code>xgboost</code> as <code>xgb</code>.
</li>
<li>
Create training and test sets such that 20% of the data is used for
testing. Use a <code>random_state</code> of <code>123</code>.
</li>
<li>
Instantiate an <code>XGBoostClassifier</code> as <code>xg_cl</code>
using <code>xgb.XGBClassifier()</code>. Specify
<code>n_estimators</code> to be <code>10</code> estimators and an
<code>objective</code> of <code>‘binary:logistic’</code>. Do not worry
about what this means just yet, you will learn about these parameters
later in this course.
</li>
<li>
Fit <code>xg_cl</code> to the training set (<code>X_train,
y_train)</code> using the <code>.fit()</code> method.
</li>
<li>
Predict the labels of the test set (<code>X_test</code>) using the
<code>.predict()</code> method and hit ‘Submit Answer’ to print the
accuracy.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># edited/added</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">churn_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;archive/Extreme-Gradient-Boosting-with-XGBoost/datasets/churn_data.csv&quot;</span><span class="p">)</span>

<span class="c1"># import xgboost</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>

<span class="c1"># Create arrays for the features and the target: X, y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">churn_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">churn_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Create the training and test sets</span>
<span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Instantiate the XGBClassifier: xg_cl</span>
<span class="n">xg_cl</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Fit the classifier to the training set</span>
<span class="n">xg_cl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict the labels of the test set: preds</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## XGBClassifier(n_estimators=10, seed=123)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">xg_cl</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute the accuracy: accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">preds</span><span class="o">==</span><span class="n">y_test</span><span class="p">))</span><span class="o">/</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## accuracy: 0.743300
</pre></div>
</div>
<p class>
Well done! Your model has an accuracy of around 74%. In Chapter 3,
you’ll learn about ways to fine tune your XGBoost models. For now, let’s
refresh our memories on how decision trees work. See you in the next
video!
</p>
</section>
</section>
<section id="what-is-a-decision-tree">
<h2>What is a decision tree?<a class="headerlink" href="#what-is-a-decision-tree" title="Permalink to this headline">#</a></h2>
<section id="decision-trees">
<h3>Decision trees<a class="headerlink" href="#decision-trees" title="Permalink to this headline">#</a></h3>
<p>
Your task in this exercise is to make a simple decision tree using
scikit-learn’s <code>DecisionTreeClassifier</code> on the <code>breast
cancer</code> dataset that comes pre-loaded with scikit-learn.
</p>
<p>
This dataset contains numeric measurements of various dimensions of
individual tumors (such as perimeter and texture) from breast biopsies
and a single outcome value (the tumor is either malignant, or benign).
</p>
<p>
We’ve preloaded the dataset of samples (measurements) into
<code>X</code> and the target values per tumor into <code>y</code>. Now,
you have to split the complete dataset into training and testing sets,
and then train a <code>DecisionTreeClassifier</code>. You’ll specify a
parameter called <code>max_depth</code>. Many other parameters can be
modified within this model, and you can check all of them out
<a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier">here</a>.
</p>
<li>
Import:
<ul>
<li>
<code>train_test_split</code> from <code>sklearn.model_selection</code>.
</li>
<li>
<code>DecisionTreeClassifier</code> from <code>sklearn.tree</code>.
</li>
</ul>
</li>
<li>
Create training and test sets such that 20% of the data is used for
testing. Use a <code>random_state</code> of <code>123</code>.
</li>
<li>
Instantiate a <code>DecisionTreeClassifier</code> called
<code>dt_clf_4</code> with a <code>max_depth</code> of <code>4</code>.
This parameter specifies the maximum number of successive split points
you can have before reaching a leaf node.
</li>
<li>
Fit the classifier to the training set and predict the labels of the
test set.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># edited/added</span>
<span class="n">breast_cancer</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;archive/Extreme-Gradient-Boosting-with-XGBoost/datasets/breast_cancer.csv&quot;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">breast_cancer</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">2</span><span class="p">:]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="s2">&quot;M&quot;</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">breast_cancer</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Import the necessary modules</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="c1"># Create the training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Instantiate the classifier: dt_clf_4</span>
<span class="n">dt_clf_4</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Fit the classifier to the training set</span>
<span class="n">dt_clf_4</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict the labels of the test set: y_pred_4</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## DecisionTreeClassifier(max_depth=4)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred_4</span> <span class="o">=</span> <span class="n">dt_clf_4</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute the accuracy of the predictions: accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_pred_4</span><span class="o">==</span><span class="n">y_test</span><span class="p">))</span><span class="o">/</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## accuracy: 0.9736842105263158
</pre></div>
</div>
<p class>
Great work! It’s now time to learn about what gives XGBoost its
state-of-the-art performance: Boosting.
</p>
</section>
</section>
<section id="what-is-boosting">
<h2>What is Boosting?<a class="headerlink" href="#what-is-boosting" title="Permalink to this headline">#</a></h2>
<section id="measuring-accuracy">
<h3>Measuring accuracy<a class="headerlink" href="#measuring-accuracy" title="Permalink to this headline">#</a></h3>
<p>
You’ll now practice using XGBoost’s learning API through its baked in
cross-validation capabilities. As Sergey discussed in the previous
video, XGBoost gets its lauded performance and efficiency gains by
utilizing its own optimized data structure for datasets called a
<code>DMatrix</code>.
</p>
<p>
In the previous exercise, the input datasets were converted into
<code>DMatrix</code> data on the fly, but when you use the
<code>xgboost</code> <code>cv</code> object, you have to first
explicitly convert your data into a <code>DMatrix</code>. So, that’s
what you will do here before running cross-validation on
<code>churn_data</code>.
</p>
<li>
Create a <code>DMatrix</code> called <code>churn_dmatrix</code> from
<code>churn_data</code> using <code>xgb.DMatrix()</code>. The features
are available in <code>X</code> and the labels in <code>y</code>.
</li>
<li>
Perform 3-fold cross-validation by calling <code>xgb.cv()</code>.
<code>dtrain</code> is your <code>churn_dmatrix</code>,
<code>params</code> is your parameter dictionary, <code>nfold</code> is
the number of cross-validation folds (<code>3</code>),
<code>num_boost_round</code> is the number of trees we want to build
(<code>5</code>), <code>metrics</code> is the metric you want to compute
(this will be <code>“error”</code>, which we will convert to an
accuracy).
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create arrays for the features and the target: X, y</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">churn_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">churn_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Create the DMatrix from X and y: churn_dmatrix</span>
<span class="n">churn_dmatrix</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Create the parameter dictionary: params</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;objective&quot;</span><span class="p">:</span><span class="s2">&quot;reg:logistic&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">:</span><span class="mi">3</span><span class="p">}</span>

<span class="c1"># Perform cross-validation: cv_results</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">dtrain</span><span class="o">=</span><span class="n">churn_dmatrix</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> 
                    <span class="n">nfold</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                    <span class="n">metrics</span><span class="o">=</span><span class="s2">&quot;error&quot;</span><span class="p">,</span> <span class="n">as_pandas</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Print cv_results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv_results</span><span class="p">)</span>

<span class="c1"># Print the accuracy</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>##    train-error-mean  train-error-std  test-error-mean  test-error-std
## 0           0.28232         0.002366          0.28378        0.001932
## 1           0.26951         0.001855          0.27190        0.001932
## 2           0.25605         0.003213          0.25798        0.003963
## 3           0.25090         0.001845          0.25434        0.003827
## 4           0.24654         0.001981          0.24852        0.000934
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(((</span><span class="mi">1</span><span class="o">-</span><span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;test-error-mean&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## 0.75148
</pre></div>
</div>
<p class>
Nice work. <code>cv_results</code> stores the training and test mean and
standard deviation of the error per boosting round (tree built) as a
DataFrame. From <code>cv_results</code>, the final round
<code>‘test-error-mean’</code> is extracted and converted into an
accuracy, where accuracy is <code>1-error</code>. The final accuracy of
around 75% is an improvement from earlier!
</p>
</section>
<section id="measuring-auc">
<h3>Measuring AUC<a class="headerlink" href="#measuring-auc" title="Permalink to this headline">#</a></h3>
<p>
Now that you’ve used cross-validation to compute average out-of-sample
accuracy (after converting from an error), it’s very easy to compute any
other metric you might be interested in. All you have to do is pass it
(or a list of metrics) in as an argument to the <code>metrics</code>
parameter of <code>xgb.cv()</code>.
</p>
<p>
Your job in this exercise is to compute another common metric used in
binary classification - the area under the curve (<code>“auc”</code>).
As before, <code>churn_data</code> is available in your workspace, along
with the DMatrix <code>churn_dmatrix</code> and parameter dictionary
<code>params</code>.
</p>
<li>
Perform 3-fold cross-validation with <code>5</code> boosting rounds and
<code>“auc”</code> as your metric.
</li>
<li>
Print the <code>“test-auc-mean”</code> column of
<code>cv_results</code>.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform cross_validation: cv_results</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">dtrain</span><span class="o">=</span><span class="n">churn_dmatrix</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> 
                    <span class="n">nfold</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                    <span class="n">metrics</span><span class="o">=</span><span class="s2">&quot;auc&quot;</span><span class="p">,</span> <span class="n">as_pandas</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Print cv_results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv_results</span><span class="p">)</span>

<span class="c1"># Print the AUC</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>##    train-auc-mean  train-auc-std  test-auc-mean  test-auc-std
## 0        0.768893       0.001544       0.767863      0.002820
## 1        0.790864       0.006758       0.789157      0.006846
## 2        0.815872       0.003900       0.814476      0.005997
## 3        0.822959       0.002018       0.821682      0.003912
## 4        0.827528       0.000769       0.826191      0.001937
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">((</span><span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;test-auc-mean&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## 0.826191
</pre></div>
</div>
<p class>
Fantastic! An AUC of 0.84 is quite strong. As you have seen, XGBoost’s
learning API makes it very easy to compute any metric you may be
interested in. In Chapter 3, you’ll learn about techniques to fine-tune
your XGBoost models to improve their performance even further. For now,
it’s time to learn a little about exactly <strong>when</strong> to use
XGBoost.
</p>
</section>
</section>
<section id="when-should-i-use-xgboost">
<h2>When should I use XGBoost?<a class="headerlink" href="#when-should-i-use-xgboost" title="Permalink to this headline">#</a></h2>
<section id="using-xgboost">
<h3>Using XGBoost<a class="headerlink" href="#using-xgboost" title="Permalink to this headline">#</a></h3>
<p>
XGBoost is a powerful library that scales very well to many samples and
works for a variety of supervised learning problems. That said, as
Sergey described in the video, you shouldn’t always pick it as your
default machine learning library when starting a new project, since
there are some situations in which it is not the best option. In this
exercise, your job is to consider the below examples and select the one
which would be the best use of XGBoost.
</p>
<li>
Visualizing the similarity between stocks by comparing the time series
of their historical prices relative to each other.
</li>
<li>
Predicting whether a person will develop cancer using genetic data with
millions of genes, 23 examples of genomes of people that didn’t develop
cancer, 3 genomes of people who wound up getting cancer.
</li>
<li>
Clustering documents into topics based on the terms used in them.
</li>
<strong>
<li>
Predicting the likelihood that a given user will click an ad from a very
large clickstream log with millions of users and their web interactions.
</li>
</strong>
<p class="dc-completion-pane__message dc-u-maxw-100pc">
Correct! Way to end the chapter. Time to apply XGBoost to solve
regression problems!
</p></section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Extreme-Gradient-Boosting-with-XGBoost-0.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Extreme Gradient Boosting with XGBoost</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Extreme-Gradient-Boosting-with-XGBoost-2.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Regression with XGBoost</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book Community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>