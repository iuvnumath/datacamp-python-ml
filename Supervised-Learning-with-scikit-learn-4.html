
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Preprocessing and Pipelines &#8212; Machine Learning Scientist with Python</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Unsupervised Learning" href="Unsupervised-Learning-in-Python-0.html" />
    <link rel="prev" title="Fine-Tuning Your Model" href="Supervised-Learning-with-scikit-learn-3.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Scientist with Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="Supervised-Learning-with-scikit-learn-0.html">
   Supervised Learning with scikit-learn
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="Supervised-Learning-with-scikit-learn-1.html">
     Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Supervised-Learning-with-scikit-learn-2.html">
     Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Supervised-Learning-with-scikit-learn-3.html">
     Fine-Tuning Your Model
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Preprocessing and Pipelines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Unsupervised-Learning-in-Python-0.html">
   Unsupervised Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Unsupervised-Learning-in-Python-1.html">
     Clustering for dataset exploration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Unsupervised-Learning-in-Python-2.html">
     Visualization with hierarchical clustering and t-SNE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Unsupervised-Learning-in-Python-3.html">
     Decorrelating your data and dimension reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Unsupervised-Learning-in-Python-4.html">
     Discovering interpretable features
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Linear-Classifiers-in-Python-0.html">
   Linear Classifiers in Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear-Classifiers-in-Python-1.html">
     Applying logistic regression and SVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear-Classifiers-in-Python-2.html">
     Loss functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear-Classifiers-in-Python-3.html">
     Logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear-Classifiers-in-Python-4.html">
     Support Vector Machines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-0.html">
   Machine Learning with Tree-Based Models in Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-1.html">
     Classification and Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-2.html">
     The Bias-Variance Tradeoff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-3.html">
     Bagging and Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-4.html">
     Boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Machine-Learning-with-Tree-Based-Models-in-Python-5.html">
     Model Tuning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Extreme-Gradient-Boosting-with-XGBoost-0.html">
   Extreme Gradient Boosting with XGBoost
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Extreme-Gradient-Boosting-with-XGBoost-1.html">
     Classification with XGBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Extreme-Gradient-Boosting-with-XGBoost-2.html">
     Regression with XGBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Extreme-Gradient-Boosting-with-XGBoost-3.html">
     Fine-tuning your XGBoost model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Extreme-Gradient-Boosting-with-XGBoost-4.html">
     Using XGBoost in pipelines
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/iuvnumath/datacamp-python-ml"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/iuvnumath/datacamp-python-ml/issues/new?title=Issue%20on%20page%20%2FSupervised-Learning-with-scikit-learn-4.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/Supervised-Learning-with-scikit-learn-4.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preprocessing-data">
   Preprocessing data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-dummy-variables">
     Creating dummy variables
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-with-categorical-features">
     Regression with categorical features
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#handling-missing-data">
   Handling missing data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropping-missing-data">
     Dropping missing data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipeline-for-song-genre-prediction-i">
     Pipeline for song genre prediction: I
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipeline-for-song-genre-prediction-ii">
     Pipeline for song genre prediction: II
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#centering-and-scaling">
   Centering and scaling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#centering-and-scaling-for-regression">
     Centering and scaling for regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#centering-and-scaling-for-classification">
     Centering and scaling for classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluating-multiple-models">
   Evaluating multiple models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-regression-model-performance">
     Visualizing regression model performance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predicting-on-the-test-set">
     Predicting on the test set
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-classification-model-performance">
     Visualizing classification model performance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipeline-for-predicting-song-popularity">
     Pipeline for predicting song popularity
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#congratulations">
   Congratulations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Congratulations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-youve-covered">
     What you’ve covered
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     What you’ve covered
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#where-to-go-from-here">
     Where to go from here?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#thank-you">
     Thank you!
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Preprocessing and Pipelines</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preprocessing-data">
   Preprocessing data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-dummy-variables">
     Creating dummy variables
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-with-categorical-features">
     Regression with categorical features
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#handling-missing-data">
   Handling missing data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropping-missing-data">
     Dropping missing data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipeline-for-song-genre-prediction-i">
     Pipeline for song genre prediction: I
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipeline-for-song-genre-prediction-ii">
     Pipeline for song genre prediction: II
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#centering-and-scaling">
   Centering and scaling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#centering-and-scaling-for-regression">
     Centering and scaling for regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#centering-and-scaling-for-classification">
     Centering and scaling for classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluating-multiple-models">
   Evaluating multiple models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-regression-model-performance">
     Visualizing regression model performance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predicting-on-the-test-set">
     Predicting on the test set
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-classification-model-performance">
     Visualizing classification model performance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipeline-for-predicting-song-popularity">
     Pipeline for predicting song popularity
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#congratulations">
   Congratulations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Congratulations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-youve-covered">
     What you’ve covered
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     What you’ve covered
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#where-to-go-from-here">
     Where to go from here?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#thank-you">
     Thank you!
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="preprocessing-and-pipelines">
<h1>Preprocessing and Pipelines<a class="headerlink" href="#preprocessing-and-pipelines" title="Permalink to this headline">#</a></h1>
<p>Learn how to impute missing values, convert categorical data to numeric
values, scale data, evaluate multiple supervised learning models
simultaneously, and build pipelines to streamline your workflow!</p>
<section id="preprocessing-data">
<h2>Preprocessing data<a class="headerlink" href="#preprocessing-data" title="Permalink to this headline">#</a></h2>
<section id="creating-dummy-variables">
<h3>Creating dummy variables<a class="headerlink" href="#creating-dummy-variables" title="Permalink to this headline">#</a></h3>
<p>
Being able to include categorical features in the model building process
can enhance performance as they may add information that contributes to
prediction accuracy.
</p>
<p>
The <code>music_df</code> dataset has been preloaded for you, and its
shape is printed. Also, <code>pandas</code> has been imported as
<code>pd</code>.
</p>
<p>
Now you will create a new DataFrame containing the original columns of
<code>music_df</code> plus dummy variables from the <code>“genre”</code>
column.
</p>
<li>
Use a relevant function, passing the entire <code>music_df</code>
DataFrame, to create <code>music_dummies</code>, dropping the first
binary column.
</li>
<li>
Print the shape of <code>music_dummies</code>.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># edited/added</span>
<span class="n">music_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;archive/Supervised-Learning-with-scikit-learn/datasets/music_clean.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Create music_dummies</span>
<span class="n">music_dummies</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">music_df</span><span class="p">,</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Print the new DataFrame&#39;s shape</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of music_dummies: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">music_dummies</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## Shape of music_dummies: (1000, 12)
</pre></div>
</div>
<p class>
As there were ten values in the <code>“genre”</code> column, nine new
columns were added by a call of <code>pd.get_dummies()</code> using
<code>drop_first=True</code>. After dropping the original
<code>“genre”</code> column, there are still eight new columns in the
DataFrame!
</p>
</section>
<section id="regression-with-categorical-features">
<h3>Regression with categorical features<a class="headerlink" href="#regression-with-categorical-features" title="Permalink to this headline">#</a></h3>
<p>
Now you have created <code>music_dummies</code>, containing binary
features for each song’s genre, it’s time to build a ridge regression
model to predict song popularity.
</p>
<p>
<code>music_dummies</code> has been preloaded for you, along with
<code>Ridge</code>, <code>cross_val_score</code>, <code>numpy</code> as
<code>np</code>, and a <code>KFold</code> object stored as
<code>kf</code>.
</p>
<p>
The model will be evaluated by calculating the average RMSE, but first,
you will need to convert the scores for each fold to positive values and
take their square root. This metric shows the average error of our
model’s predictions, so it can be compared against the standard
deviation of the target value—<code>“popularity”</code>.
</p>
<li>
Create <code>X</code>, containing all features in
<code>music_dummies</code>, and <code>y</code>, consisting of the
<code>“popularity”</code> column, respectively.
</li>
<li>
Instantiate a ridge regression model, setting <code>alpha</code> equal
to 0.2.
</li>
<li>
Perform cross-validation on <code>X</code> and <code>y</code> using the
ridge model, setting <code>cv</code> equal to <code>kf</code>, and using
negative mean squared error as the scoring metric.
</li>
<li>
Print the RMSE values by converting negative <code>scores</code> to
positive and taking the square root.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create X and y</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">music_dummies</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;popularity&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">music_dummies</span><span class="p">[</span><span class="s2">&quot;popularity&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Instantiate a ridge model</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Perform cross-validation</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">ridge</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kf</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="p">)</span>

<span class="c1"># Calculate RMSE</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="n">scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average RMSE: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rmse</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## Average RMSE: 10.356167918309263
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Standard Deviation of the target array: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## Standard Deviation of the target array: 14.02156909907019
</pre></div>
</div>
<p class>
Great work! An average RMSE of approximately <code>8.24</code> is lower
than the standard deviation of the target variable (song popularity),
suggesting the model is reasonably accurate.
</p>
</section>
</section>
<section id="handling-missing-data">
<h2>Handling missing data<a class="headerlink" href="#handling-missing-data" title="Permalink to this headline">#</a></h2>
<section id="dropping-missing-data">
<h3>Dropping missing data<a class="headerlink" href="#dropping-missing-data" title="Permalink to this headline">#</a></h3>
<p>
Over the next three exercises, you are going to tidy the
<code>music_df</code> dataset. You will create a pipeline to impute
missing values and build a KNN classifier model, then use it to predict
whether a song is of the <code>“Rock”</code> genre.
</p>
<p>
In this exercise specifically, you will drop missing values accounting
for less than 5% of the dataset, and convert the <code>“genre”</code>
column into a binary feature.
</p>
<li>
Print the number of missing values for each column in the
<code>music_df</code> dataset, sorted in ascending order.
</li>
<li>
Remove values for all columns with 50 or fewer missing values.
</li>
<li>
Convert <code>music_df\[“genre”\]</code> to values of <code>1</code> if
the row contains <code>“Rock”</code>, otherwise change the value to
<code>0</code>.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print missing values for each column</span>
<span class="nb">print</span><span class="p">(</span><span class="n">music_df</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">())</span>

<span class="c1"># Remove values where less than 5% are missing</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## popularity          0
## acousticness        0
## danceability        0
## duration_ms         0
## energy              0
## instrumentalness    0
## liveness            0
## loudness            0
## speechiness         0
## tempo               0
## valence             0
## genre               0
## dtype: int64
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">music_df</span> <span class="o">=</span> <span class="n">music_df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;genre&quot;</span><span class="p">,</span> <span class="s2">&quot;popularity&quot;</span><span class="p">,</span> <span class="s2">&quot;loudness&quot;</span><span class="p">,</span> <span class="s2">&quot;liveness&quot;</span><span class="p">,</span> <span class="s2">&quot;tempo&quot;</span><span class="p">])</span>

<span class="c1"># Convert genre to a binary feature</span>
<span class="n">music_df</span><span class="p">[</span><span class="s2">&quot;genre&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">music_df</span><span class="p">[</span><span class="s2">&quot;genre&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Rock&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">music_df</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## popularity          0
## acousticness        0
## danceability        0
## duration_ms         0
## energy              0
## instrumentalness    0
## liveness            0
## loudness            0
## speechiness         0
## tempo               0
## valence             0
## genre               0
## dtype: int64
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of the `music_df`: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">music_df</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## Shape of the `music_df`: (1000, 12)
</pre></div>
</div>
<p class>
Well done! The dataset has gone from 1000 observations down to 892, but
it is now in the correct format for binary classification and the
remaining missing values can be imputed as part of a pipeline.
</p>
</section>
<section id="pipeline-for-song-genre-prediction-i">
<h3>Pipeline for song genre prediction: I<a class="headerlink" href="#pipeline-for-song-genre-prediction-i" title="Permalink to this headline">#</a></h3>
<p>
Now it’s time to build a pipeline. It will contain steps to impute
missing values using the mean for each feature and build a KNN model for
the classification of song genre.
</p>
<p>
The modified <code>music_df</code> dataset that you created in the
previous exercise has been preloaded for you, along with
<code>KNeighborsClassifier</code> and <code>train_test_split</code>.
</p>
<li>
Import <code>SimpleImputer</code> and <code>Pipeline</code>.
</li>
<li>
Instantiate an imputer.
</li>
<li>
Instantiate a KNN classifier with three neighbors.
</li>
<li>
Create <code>steps</code>, a list of tuples containing the imputer
variable you created, called <code>“imputer”</code>, followed by the
<code>knn</code> model you created, called <code>“knn”</code>.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import modules</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># Instantiate an imputer</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">()</span>

<span class="c1"># Instantiate a knn model</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Build steps for the pipeline</span>
<span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">imputer</span><span class="p">),</span> 
         <span class="p">(</span><span class="s2">&quot;knn&quot;</span><span class="p">,</span> <span class="n">knn</span><span class="p">)]</span>
</pre></div>
</div>
<p class>
Perfect pipeline skills! You are now ready to build and evaluate a song
genre classification model.
</p>
</section>
<section id="pipeline-for-song-genre-prediction-ii">
<h3>Pipeline for song genre prediction: II<a class="headerlink" href="#pipeline-for-song-genre-prediction-ii" title="Permalink to this headline">#</a></h3>
<p>
Having set up the steps of the pipeline in the previous exercise, you
will now use it on the <code>music_df</code> dataset to classify the
genre of songs. What makes pipelines so incredibly useful is the simple
interface that they provide.
</p>
<p>
<code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and
<code>y_test</code> have been preloaded for you, and
<code>confusion_matrix</code> has been imported from
<code>sklearn.metrics</code>.
</p>
<li>
Create a pipeline using the steps you previously defined.
</li>
<li>
Fit the pipeline to the training data.
</li>
<li>
Make predictions on the test set.
</li>
<li>
Calculate and print the confusion matrix.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># edited/added</span>
<span class="n">imp_mean</span> <span class="o">=</span> <span class="n">imputer</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">21</span><span class="p">)</span>

<span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">imp_mean</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;knn&quot;</span><span class="p">,</span> <span class="n">knn</span><span class="p">)]</span>

<span class="c1"># Create the pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>

<span class="c1"># Fit the pipeline to the training data</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions on the test set</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()),
##                 (&#39;knn&#39;, KNeighborsClassifier(n_neighbors=3))])
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Print the confusion matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## [[0 0 0 ... 0 0 0]
##  [0 0 0 ... 0 0 0]
##  [0 0 0 ... 0 0 0]
##  ...
##  [0 0 0 ... 0 0 0]
##  [0 0 0 ... 0 0 0]
##  [0 0 0 ... 0 0 0]]
</pre></div>
</div>
<p class>
Excellent! See how easy it is to scale our model building workflow using
pipelines. In this case, the confusion matrix highlights that the model
had 79 true positives and 82 true negatives!
</p>
</section>
</section>
<section id="centering-and-scaling">
<h2>Centering and scaling<a class="headerlink" href="#centering-and-scaling" title="Permalink to this headline">#</a></h2>
<section id="centering-and-scaling-for-regression">
<h3>Centering and scaling for regression<a class="headerlink" href="#centering-and-scaling-for-regression" title="Permalink to this headline">#</a></h3>
<p>
Now you have seen the benefits of scaling your data, you will use a
pipeline to preprocess the <code>music_df</code> features and build a
lasso regression model to predict a song’s loudness.
</p>
<p>
<code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and
<code>y_test</code> have been created from the <code>music_df</code>
dataset, where the target is <code>“loudness”</code> and the features
are all other columns in the dataset. <code>Lasso</code> and
<code>Pipeline</code> have also been imported for you.
</p>
<p>
Note that <code>“genre”</code> has been converted to a binary feature
where <code>1</code> indicates a rock song, and <code>0</code>
represents other genres.
</p>
<li>
Import <code>StandardScaler</code>.
</li>
<li>
Create the steps for the pipeline object, a <code>StandardScaler</code>
object called <code>“scaler”</code>, and a lasso model called
<code>“lasso”</code> with <code>alpha</code> set to <code>0.5</code>.
</li>
<li>
Instantiate a pipeline with steps to scale and build a lasso regression
model.
</li>
<li>
Calculate the R-squared value on the test data.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Create pipeline steps</span>
<span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
         <span class="p">(</span><span class="s2">&quot;lasso&quot;</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))]</span>
         
<span class="c1"># Instantiate the pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Calculate and print R-squared</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## Pipeline(steps=[(&#39;scaler&#39;, StandardScaler()), (&#39;lasso&#39;, Lasso(alpha=0.5))])
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## 0.47454082360792205
</pre></div>
</div>
<p class>
Awesome scaling! The model may have only produced an R-squared of
<code>0.619</code>, but without scaling this exact model would have only
produced a score of <code>0.35</code>, which proves just how powerful
scaling can be!
</p>
</section>
<section id="centering-and-scaling-for-classification">
<h3>Centering and scaling for classification<a class="headerlink" href="#centering-and-scaling-for-classification" title="Permalink to this headline">#</a></h3>
<p>
Now you will bring together scaling and model building into a pipeline
for cross-validation.
</p>
<p>
Your task is to build a pipeline to scale features in the
<code>music_df</code> dataset and perform grid search cross-validation
using a logistic regression model with different values for the
hyperparameter <code>C</code>. The target variable here is
<code>“genre”</code>, which contains binary values for rock as
<code>1</code> and any other genre as <code>0</code>.
</p>
<p>
<code>StandardScaler</code>, <code>LogisticRegression</code>, and
<code>GridSearchCV</code> have all been imported for you.
</p>
<li>
Build the steps for the pipeline: a <code>StandardScaler()</code> object
named <code>“scaler”</code>, and a logistic regression model named
<code>“logreg”</code>.
</li>
<li>
Create the <code>parameters</code>, searching 20 equally spaced float
values ranging from <code>0.001</code> to <code>1.0</code> for the
logistic regression model’s <code>C</code> hyperparameter within the
pipeline.
</li>
<li>
Instantiate the grid search object.
</li>
<li>
Fit the grid search object to the training data.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build the steps</span>
<span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
         <span class="p">(</span><span class="s2">&quot;logreg&quot;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">())]</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>

<span class="c1"># Create the parameter space</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;logreg__C&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)}</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">21</span><span class="p">)</span>
                                                    
<span class="c1"># Instantiate the grid search object</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">parameters</span><span class="p">)</span>

<span class="c1"># Fit to the training data</span>
<span class="n">cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## GridSearchCV(estimator=Pipeline(steps=[(&#39;scaler&#39;, StandardScaler()),
##                                        (&#39;logreg&#39;, LogisticRegression())]),
##              param_grid={&#39;logreg__C&#39;: array([0.001     , 0.05357895, 0.10615789, 0.15873684, 0.21131579,
##        0.26389474, 0.31647368, 0.36905263, 0.42163158, 0.47421053,
##        0.52678947, 0.57936842, 0.63194737, 0.68452632, 0.73710526,
##        0.78968421, 0.84226316, 0.89484211, 0.94742105, 1.        ])})
## 
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.
##   warnings.warn((&quot;The least populated class in y has only %d&quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## 0.052500000000000005 
##  {&#39;logreg__C&#39;: 0.1061578947368421}
</pre></div>
</div>
<p class>
Well done! Using a pipeline shows that a logistic regression model with
<code>“C”</code> set to approximately <code>0.1</code> produces a model
with <code>0.8425</code> accuracy!
</p>
</section>
</section>
<section id="evaluating-multiple-models">
<h2>Evaluating multiple models<a class="headerlink" href="#evaluating-multiple-models" title="Permalink to this headline">#</a></h2>
<section id="visualizing-regression-model-performance">
<h3>Visualizing regression model performance<a class="headerlink" href="#visualizing-regression-model-performance" title="Permalink to this headline">#</a></h3>
<p>
Now you have seen how to evaluate multiple models out of the box, you
will build three regression models to predict a song’s
<code>“energy”</code> levels.
</p>
<p>
The <code>music_df</code> dataset has had dummy variables for
<code>“genre”</code> added. Also, feature and target arrays have been
created, and these have been split into <code>X_train</code>,
<code>X_test</code>, <code>y_train</code>, and <code>y_test</code>.
</p>
<p>
The following have been imported for you: <code>LinearRegression</code>,
<code>Ridge</code>, <code>Lasso</code>, <code>cross_val_score</code>,
and <code>KFold</code>.
</p>
<li>
Write a for loop using <code>model</code> as the iterator, and
<code>model.values()</code> as the iterable.
</li>
<li>
Perform cross-validation on the training features and the training
target array using the model, setting <code>cv</code> equal to the
<code>KFold</code> object.
</li>
<li>
Append the model’s cross-validation scores to the results list.
</li>
<li>
Create a box plot displaying the results, with the x-axis labels as the
names of the models.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Linear Regression&quot;</span><span class="p">:</span> <span class="n">LinearRegression</span><span class="p">(),</span> <span class="s2">&quot;Ridge&quot;</span><span class="p">:</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span> <span class="s2">&quot;Lasso&quot;</span><span class="p">:</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)}</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop through the models&#39; values</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
  <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  
  <span class="c1"># Perform cross-validation</span>
  <span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kf</span><span class="p">)</span>
  
  <span class="c1"># Append the results</span>
  <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)</span>
  
<span class="c1"># Create a box plot of the results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">models</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7ffca8ceaa60&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8ceadf0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d2a2b0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d2a640&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d35be0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d35f70&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D object at 0x7ffca8cff0a0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8cff430&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d2a9d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d2ad60&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d3f340&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d3f6d0&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D object at 0x7ffca8cd9c40&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8cffee0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d35850&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D object at 0x7ffca8cff7c0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d35130&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d3fa60&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7ffca8cffb50&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d354c0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca8d3fdf0&gt;], &#39;means&#39;: []}
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="Supervised-Learning-with-scikit-learn_files/figure-markdown_github/unnamed-chunk-31-9.png" width="672" />
<p class>
Nicely done! Lasso regression is not a good model for this problem,
while linear regression and ridge perform fairly equally. Let’s make
predictions on the test set, and see if the RMSE can guide us on model
selection.
</p>
</section>
<section id="predicting-on-the-test-set">
<h3>Predicting on the test set<a class="headerlink" href="#predicting-on-the-test-set" title="Permalink to this headline">#</a></h3>
<p>
In the last exercise, linear regression and ridge appeared to produce
similar results. It would be appropriate to select either of those
models; however, you can check predictive performance on the test set to
see if either one can outperform the other.
</p>
<p>
You will use root mean squared error (RMSE) as the metric. The
dictionary <code>models</code>, containing the names and instances of
the two models, has been preloaded for you along with the training and
target arrays <code>X_train_scaled</code>, <code>X_test_scaled</code>,
<code>y_train</code>, and <code>y_test</code>.
</p>
<li>
Import <code>mean_squared_error</code>.
</li>
<li>
Fit the model to the scaled training features and the training labels.
</li>
<li>
Make predictions using the scaled test features.
</li>
<li>
Calculate RMSE by passing the test set labels and the predicted labels.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># edited/added</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">21</span><span class="p">)</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Import mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
  
  <span class="c1"># Fit the model to the training data</span>
  <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  
  <span class="c1"># Make predictions on the test set</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
  
  <span class="c1"># Calculate the test_rmse</span>
  <span class="n">test_rmse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> Test Set RMSE: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">test_rmse</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## LinearRegression()
## Linear Regression Test Set RMSE: 10.368037372390296
## Ridge(alpha=0.1)
## Ridge Test Set RMSE: 10.367981364747633
## Lasso(alpha=0.1)
## Lasso Test Set RMSE: 10.354423827065155
</pre></div>
</div>
<p class>
The linear regression model just edges the best performance, although
the difference is a RMSE of 0.00001 for popularity! Now let’s look at
classification model selection.
</p>
</section>
<section id="visualizing-classification-model-performance">
<h3>Visualizing classification model performance<a class="headerlink" href="#visualizing-classification-model-performance" title="Permalink to this headline">#</a></h3>
<p>
In this exercise, you will be solving a classification problem where the
<code>“popularity”</code> column in the <code>music_df</code> dataset
has been converted to binary values, with <code>1</code> representing
popularity more than or equal to the median for the
<code>“popularity”</code> column, and <code>0</code> indicating
popularity below the median.
</p>
<p>
Your task is to build and visualize the results of three different
models to classify whether a song is popular or not.
</p>
<p>
The data has been split, scaled, and preloaded for you as
<code>X_train_scaled</code>, <code>X_test_scaled</code>,
<code>y_train</code>, and <code>y_test</code>. Additionally,
<code>KNeighborsClassifier</code>, <code>DecisionTreeClassifier</code>,
and <code>LogisticRegression</code> have been imported.
</p>
<li>
Create a dictionary of <code>“Logistic Regression”</code>,
<code>“KNN”</code>, and <code>“Decision Tree Classifier”</code>, setting
the dictionary’s values to a call of each model.
</li>
<li>
Loop through the values in <code>models</code>.
</li>
<li>
Instantiate a <code>KFold</code> object to perform 6 splits, setting
<code>shuffle</code> to <code>True</code> and <code>random_state</code>
to <code>12</code>.
</li>
<li>
Perform cross-validation using the model, the scaled training features,
the target training set, and setting <code>cv</code> equal to
<code>kf</code>.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># edited/added</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="c1"># Create models dictionary</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Logistic Regression&quot;</span><span class="p">:</span> <span class="n">LogisticRegression</span><span class="p">(),</span> <span class="s2">&quot;KNN&quot;</span><span class="p">:</span> <span class="n">KNeighborsClassifier</span><span class="p">(),</span> <span class="s2">&quot;Decision Tree Classifier&quot;</span><span class="p">:</span> <span class="n">DecisionTreeClassifier</span><span class="p">()}</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop through the models&#39; values</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
  
  <span class="c1"># Instantiate a KFold object</span>
  <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  
  <span class="c1"># Perform cross-validation</span>
  <span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kf</span><span class="p">)</span>
  <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv_results</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">models</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7ffca68f9d30&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca68f9fa0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca6910580&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca6910910&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca2d8beb0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca2d94280&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D object at 0x7ffca6907370&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca6907700&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca6910ca0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca2d8b070&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca2d94610&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca2d949a0&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D object at 0x7ffca68f99a0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca69101f0&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca2d8bb20&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D object at 0x7ffca6907a90&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca2d8b400&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca2d94d30&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D object at 0x7ffca6907e20&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca2d8b790&gt;, &lt;matplotlib.lines.Line2D object at 0x7ffca2da1100&gt;], &#39;means&#39;: []}
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="Supervised-Learning-with-scikit-learn_files/figure-markdown_github/unnamed-chunk-33-11.png" width="672" />
<p class>
Looks like logistic regression is the best candidate based on the
cross-validation results! Let’s wrap up by building a pipeline
</p>
</section>
<section id="pipeline-for-predicting-song-popularity">
<h3>Pipeline for predicting song popularity<a class="headerlink" href="#pipeline-for-predicting-song-popularity" title="Permalink to this headline">#</a></h3>
<p>
For the final exercise, you will build a pipeline to impute missing
values, scale features, and perform hyperparameter tuning of a logistic
regression model. The aim is to find the best parameters and accuracy
when predicting song genre!
</p>
<p>
All the models and objects required to build the pipeline have been
preloaded for you.
</p>
<li>
Create the steps for the pipeline by calling a simple imputer, a
standard scaler, and a logistic regression model.
</li>
<li>
Create a pipeline object, and pass the <code>steps</code> variable.
</li>
<li>
Instantiate a grid search object to perform cross-validation using the
pipeline and the parameters.
</li>
<li>
Print the best parameters and compute and print the test set accuracy
score for the grid search object.
</li>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create steps</span>
<span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;imp_mean&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">()),</span> 
         <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span> 
         <span class="p">(</span><span class="s2">&quot;logreg&quot;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">())]</span>
         
<span class="c1"># Set up pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;logreg__solver&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;newton-cg&quot;</span><span class="p">,</span> <span class="s2">&quot;saga&quot;</span><span class="p">,</span> <span class="s2">&quot;lbfgs&quot;</span><span class="p">],</span>
         <span class="s2">&quot;logreg__C&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)}</span>
         
<span class="c1"># Create the GridSearchCV object</span>
<span class="n">tuning</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
<span class="n">tuning</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## GridSearchCV(estimator=Pipeline(steps=[(&#39;imp_mean&#39;, SimpleImputer()),
##                                        (&#39;scaler&#39;, StandardScaler()),
##                                        (&#39;logreg&#39;, LogisticRegression())]),
##              param_grid={&#39;logreg__C&#39;: array([0.001, 0.112, 0.223, 0.334, 0.445, 0.556, 0.667, 0.778, 0.889,
##        1.   ]),
##                          &#39;logreg__solver&#39;: [&#39;newton-cg&#39;, &#39;saga&#39;, &#39;lbfgs&#39;]})
## 
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.
##   warnings.warn((&quot;The least populated class in y has only %d&quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
##   warnings.warn(&quot;The max_iter was reached which means &quot;
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
## /Users/macos/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   n_iter_i = _check_optimize_result(
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">tuning</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Compute and print performance</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tuned Logistic Regression Parameters: </span><span class="si">{}</span><span class="s2">, Accuracy: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tuning</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">tuning</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## Tuned Logistic Regression Parameters: {&#39;logreg__C&#39;: 0.112, &#39;logreg__solver&#39;: &#39;newton-cg&#39;}, Accuracy: 0.056
</pre></div>
</div>
<p class>
Excellent - you’ve selected a model, built a preprocessing pipeline, and
performed hyperparameter tuning to create a model that is 82% accurate
in predicting song genres!
</p>
</section>
</section>
<section id="congratulations">
<h2>Congratulations<a class="headerlink" href="#congratulations" title="Permalink to this headline">#</a></h2>
<section id="id1">
<h3>Congratulations<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<p>Well done on completing the course, I predicted that you would!</p>
</section>
<section id="what-youve-covered">
<h3>What you’ve covered<a class="headerlink" href="#what-youve-covered" title="Permalink to this headline">#</a></h3>
<p>To recap, you have learned the fundamentals of using supervised learning
techniques to build predictive models for both regression and
classification problems. You have learned the concepts of underfitting
and overfitting, how to split data, and perform cross-validation.</p>
</section>
<section id="id2">
<h3>What you’ve covered<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
<p>You also learned about data preprocessing techniques, selected which
model to build, performed hyperparameter tuning, assessed model
performance, and used pipelines!</p>
</section>
<section id="where-to-go-from-here">
<h3>Where to go from here?<a class="headerlink" href="#where-to-go-from-here" title="Permalink to this headline">#</a></h3>
<p>We covered several models, but there are plenty of others, so to learn
more we recommend checking out some of our courses. We also have courses
that dive deeper into topics we introduced, such as preprocessing, or
model validation. There are other courses on topics we did not cover,
such as feature engineering, and unsupervised learning. Additionally, we
have many machine learning projects where you can apply the skills
you’ve learned here!</p>
</section>
<section id="thank-you">
<h3>Thank you!<a class="headerlink" href="#thank-you" title="Permalink to this headline">#</a></h3>
<p>Congratulations again, and thank you for taking the course! I hope you
enjoy using scikit-learn for your supervised learning problems from now
on!</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Supervised-Learning-with-scikit-learn-3.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Fine-Tuning Your Model</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Unsupervised-Learning-in-Python-0.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Unsupervised Learning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book Community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>